{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://runawayhorse001.github.io/LearningApacheSpark/clustering.html\n",
    "from utilsom import * "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#input_data = spark.read.csv(\"ALOI_norm.csv\", header=True, inferSchema=True, mode=\"DROPMALFORMED\", encoding='UTF-8')\n",
    "#df = df.drop(\"_c0\")\n",
    "#input_data.show(n=5,truncate=False)\n",
    "#http://archive.ics.uci.edu/ml/datasets/HEPMASS  \n",
    "input_data = load_data(\"hdfs://localhost:9000/data/ALOI_norm.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50000\n",
      "DataFrame[_c0: int, att1: double, att2: double, att3: double, att4: double, att5: double, att6: double, att7: double, att8: double, att9: double, att10: double, att11: double, att12: double, att13: double, att14: double, att15: double, att16: double, att17: double, att18: double, att19: double, att20: double, att21: double, att22: double, att23: double, att24: double, att25: double, att26: double, att27: double, id: double, c90: string]\n"
     ]
    }
   ],
   "source": [
    "print(input_data.count())\n",
    "print(input_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dellet label\n",
    "\n",
    "df_input_data = input_data.drop('id','_c0','c90')\n",
    "\n",
    "# staep 2\n",
    "df_input_data = shuffling(df_input_data, 10)\n",
    "\n",
    "# staep 3\n",
    "input_features = assembler(df_input_data,df_input_data.columns[0:])\n",
    "\n",
    "# staep 4\n",
    "scaledData = scaler(input_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SOM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SOM training epoches 1\n",
      "neighborhood radius  5.015752812467621\n",
      "-------------------------------------\n",
      "SOM training epoches 2\n",
      "neighborhood radius  4.192962712629475\n",
      "-------------------------------------\n",
      "SOM training epoches 3\n",
      "neighborhood radius  3.505144086407193\n",
      "-------------------------------------\n",
      "SOM training epoches 4\n",
      "neighborhood radius  2.9301560515835217\n",
      "-------------------------------------\n",
      "SOM training epoches 5\n",
      "neighborhood radius  2.449489742783178\n",
      "-------------------------------------\n",
      "SOM training epoches 6\n",
      "neighborhood radius  2.0476725110792193\n",
      "-------------------------------------\n",
      "SOM training epoches 7\n",
      "neighborhood radius  1.7117698594097055\n",
      "-------------------------------------\n",
      "SOM training epoches 8\n",
      "neighborhood radius  1.4309690811052556\n",
      "-------------------------------------\n",
      "SOM training epoches 9\n",
      "neighborhood radius  1.1962311988513155\n",
      "-------------------------------------\n",
      "SOM training epoches 10\n",
      "neighborhood radius  1.0\n",
      "-------------------------------------\n",
      "dimention of codebook 3D: (13, 6, 27)\n",
      "dimention of codebook 2D: (78, 27)\n"
     ]
    }
   ],
   "source": [
    "#https://machinelearningnepal.com/2018/01/22/apache-spark-implementation-of-som-batch-algorithm/\n",
    "# step 5 Som\n",
    "from som.batch_som import SOM\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib import patches as patches\n",
    "\n",
    "#mapsize = calculate_map_size(input_data)\n",
    "#print('Som: X = {} and Y = {}, {} neurones'.format(mapsize[0], mapsize[1],mapsize[0]*mapsize[1]))\n",
    "\n",
    "#x =  mapsize[0] \n",
    "#y = mapsize[1]\n",
    "x =  13\n",
    "y = 6\n",
    "som = SOM(x,y,27) # (xÃ—y) neurons, (70 features)\n",
    "som.train(scaledData, 10) # data , epochs  +10\n",
    "\n",
    "   # for show All codebook\n",
    "#som.net\n",
    "   # dimention of codebook (x,y,features)\n",
    "print('dimention of codebook 3D:',som.net.shape)\n",
    "\n",
    "#som.net.reshpae(x*y,features)\n",
    "data = som.net\n",
    "data = data.transpose(2,1,0).reshape(-1,data.shape[2])\n",
    "print('dimention of codebook 2D:',data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+----------------+----------------+-----+----------------+----------------+-----+----------------+----------------+----------------+---------------+----------------+----------------+-----+----------------+----------------+---------------+----------------+----------------+----------------+----+----------------+----------------+----+----+----+----+--------------------+----------+--------------------+\n",
      "|          att1|           att10|           att11|att12|           att13|           att14|att15|           att16|           att17|           att18|          att19|            att2|           att20|att21|           att22|           att23|          att24|           att25|           att26|           att27|att3|            att4|            att5|att6|att7|att8|att9|                 bmu|   bmu_idx|            features|\n",
      "+--------------+----------------+----------------+-----+----------------+----------------+-----+----------------+----------------+----------------+---------------+----------------+----------------+-----+----------------+----------------+---------------+----------------+----------------+----------------+----+----------------+----------------+----+----+----+----+--------------------+----------+--------------------+\n",
      "|0.970369747642|0.00696450551383|2.17648156209E-4|  0.0| 0.0134313963791|0.00815110113403|  0.0|1.26187742123E-5|0.00117039953184|3.35497290859E-5|0.0022143388015|             0.0|             0.0|  0.0|9.95535485195E-4|0.00236211172788|2.0043293514E-4|2.75072894317E-4|0.00505120880652| 0.0179218881398| 0.0|7.88972942869E-4|9.62989118223E-5| 0.0| 0.0| 0.0| 0.0|[0.97341202528250...|[12.0,1.0]|[0.970369747642,0...|\n",
      "| 0.68837794048|  0.228114245522|             0.0|  0.0|0.00624132604656|5.96044432403E-4|  0.0|2.52375484245E-5| 6.9159972336E-4|             0.0| 0.319822546413|9.18923389357E-6|6.46211905808E-4|  0.0| 0.0025760763586| 0.0032425351901|            0.0|9.16909647723E-6|0.00168373626884|0.00120628093249| 0.0|0.00972873253817|9.62989118223E-5| 0.0| 0.0| 0.0| 0.0|[0.59670496187115...| [6.0,5.0]|(27,[0,1,3,4,9,12...|\n",
      "+--------------+----------------+----------------+-----+----------------+----------------+-----+----------------+----------------+----------------+---------------+----------------+----------------+-----+----------------+----------------+---------------+----------------+----------------+----------------+----+----------------+----------------+----+----+----+----+--------------------+----------+--------------------+\n",
      "only showing top 2 rows\n",
      "\n",
      "50000\n"
     ]
    }
   ],
   "source": [
    "# prediction som\n",
    "# pour avoir bmu, bmu, index \n",
    "pre = som.predict(scaledData)\n",
    "pre.show(2)\n",
    "print(pre.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----------+--------------------+\n",
      "|                 bmu|   bmu_idx|            features|\n",
      "+--------------------+----------+--------------------+\n",
      "|[0.97341202528250...|[12.0,1.0]|[0.970369747642,0...|\n",
      "|[0.59670496187115...| [6.0,5.0]|(27,[0,1,3,4,9,12...|\n",
      "|[0.92026447592050...|[12.0,2.0]|[0.935771823365,3...|\n",
      "|[0.97341202528250...|[12.0,1.0]|[0.981757180197,2...|\n",
      "|[0.64172085599527...| [0.0,1.0]|[0.423589563303,0...|\n",
      "+--------------------+----------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# to kmenas, select, bmu \n",
    "bmux = pre.select('bmu','bmu_idx','features')\n",
    "print(bmux.show(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K-means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "from pyspark.ml.feature import VectorIndexer\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "# Automatically identify categorical features, and index them.\n",
    "# We specify maxCategories so features with > 4 distinct values are treated as continuous.\n",
    "\n",
    "featureIndexer = VectorIndexer(inputCol=\"bmu\", \\\n",
    "                               outputCol=\"indexedbmu\",\\\n",
    "                               ).fit(bmux)\n",
    "\n",
    "data = featureIndexer.transform(bmux)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----------+--------------------+--------------------+\n",
      "|                 bmu|   bmu_idx|            features|          indexedbmu|\n",
      "+--------------------+----------+--------------------+--------------------+\n",
      "|[0.97341202528250...|[12.0,1.0]|[0.970369747642,0...|[0.97341202528250...|\n",
      "|[0.59670496187115...| [6.0,5.0]|(27,[0,1,3,4,9,12...|[0.59670496187115...|\n",
      "|[0.92026447592050...|[12.0,2.0]|[0.935771823365,3...|[0.92026447592050...|\n",
      "|[0.97341202528250...|[12.0,1.0]|[0.981757180197,2...|[0.97341202528250...|\n",
      "|[0.64172085599527...| [0.0,1.0]|[0.423589563303,0...|[0.64172085599527...|\n",
      "+--------------------+----------+--------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data.show(5,True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.clustering import KMeans, KMeansModel\n",
    "\n",
    "kmeans = KMeans() \\\n",
    "          .setK(2) \\\n",
    "          .setFeaturesCol(\"indexedbmu\")\\\n",
    "          .setPredictionCol(\"cluster\")\n",
    "\n",
    "# Chain indexer and tree in a Pipeline\n",
    "pipeline = Pipeline(stages=[featureIndexer, kmeans])\n",
    "\n",
    "model = pipeline.fit(bmux)\n",
    "\n",
    "cluster = model.transform(bmux)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster = model.transform(bmux)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----------+--------------------+--------------------+-------+\n",
      "|                 bmu|   bmu_idx|            features|          indexedbmu|cluster|\n",
      "+--------------------+----------+--------------------+--------------------+-------+\n",
      "|[0.97341202528250...|[12.0,1.0]|[0.970369747642,0...|[0.97341202528250...|      0|\n",
      "|[0.59670496187115...| [6.0,5.0]|(27,[0,1,3,4,9,12...|[0.59670496187115...|      1|\n",
      "|[0.92026447592050...|[12.0,2.0]|[0.935771823365,3...|[0.92026447592050...|      0|\n",
      "|[0.97341202528250...|[12.0,1.0]|[0.981757180197,2...|[0.97341202528250...|      0|\n",
      "|[0.64172085599527...| [0.0,1.0]|[0.423589563303,0...|[0.64172085599527...|      1|\n",
      "+--------------------+----------+--------------------+--------------------+-------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cluster.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = cluster.select('cluster')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "y_pred = np.array(list(cluster.select('cluster').toPandas()['cluster']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import pandas as pd\n",
    "\n",
    "# load data\n",
    "df = pd.read_csv('ALOI_norm.csv')  \n",
    "df['c90'] = df.c90.apply(lambda label: 1 if label == \"'yes'\" else 0)\n",
    "df.head()\n",
    "del df['id']\n",
    "del df['Unnamed: 0']\n",
    "# df to values\n",
    "df = df.values\n",
    "# \n",
    "y_test= df[:,27]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict\n",
    "acc = accuracy_score(y_test, y_pred)\n",
    "print(\"Test Accuracy of SOM K-means = {:.6f} %\".format(acc * 100))\n",
    "# Matrix de confusion\n",
    "print(confusion_matrix(y_test,y_pred))\n",
    "print('\\n')\n",
    "print(classification_report(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Test Accuracy of SOM K-means = 59.864000 %\n",
    "[[29272 19220]\n",
    " [  848   660]]\n",
    "\n",
    "\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "         0.0       0.97      0.60      0.74     48492\n",
    "         1.0       0.03      0.44      0.06      1508\n",
    "\n",
    "   micro avg       0.60      0.60      0.60     50000\n",
    "   macro avg       0.50      0.52      0.40     50000\n",
    "weighted avg       0.94      0.60      0.72     50000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
