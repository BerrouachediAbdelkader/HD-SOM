{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://runawayhorse001.github.io/LearningApacheSpark/clustering.html\n",
    "from utilsom import * "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1688\n",
      "DataFrame[_c0: int, LB: double, AC: double, FM: double, UC: double, ASTV: double, MSTV: double, ALTV: double, MLTV: double, DL: double, DS: double, DP: double, Width: double, Min: double, Max: double, Nmax: double, Nzeros: double, Mode: double, Mean: double, Median: double, Variance: double, Tendency: double, id: double, c90: string]\n"
     ]
    }
   ],
   "source": [
    "#input_data = spark.read.csv(\"Cardiotocography_02_v10.csv\", header=True, inferSchema=True, mode=\"DROPMALFORMED\", encoding='UTF-8')\n",
    "#df = df.drop(\"_c0\")\n",
    "#input_data.show(n=5,truncate=False)\n",
    "#http://archive.ics.uci.edu/ml/datasets/HEPMASS  \n",
    "input_data = load_data(\"hdfs://localhost:9000/data/Cardiotocography_02_v10.csv\")\n",
    "print(input_data.count())\n",
    "print(input_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dellet label\n",
    "df_input_data = input_data.drop(\"_c0\",\"c90\",\"id\")\n",
    "# staep 2\n",
    "df_input_data = shuffling(df_input_data, 10)\n",
    "\n",
    "# staep 3\n",
    "input_features = assembler(df_input_data,df_input_data.columns[0:])\n",
    "\n",
    "# staep 4\n",
    "scaledData = scaler(input_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SOM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Som: X = 15 and Y = 14, 210 neurones\n",
      "SOM training epoches 1\n",
      "neighborhood radius  4.256699612603923\n",
      "-------------------------------------\n",
      "SOM training epoches 2\n",
      "neighborhood radius  3.623898318388478\n",
      "-------------------------------------\n",
      "SOM training epoches 3\n",
      "neighborhood radius  3.0851693136000486\n",
      "-------------------------------------\n",
      "SOM training epoches 4\n",
      "neighborhood radius  2.626527804403768\n",
      "-------------------------------------\n",
      "SOM training epoches 5\n",
      "neighborhood radius  2.23606797749979\n",
      "-------------------------------------\n",
      "SOM training epoches 6\n",
      "neighborhood radius  1.9036539387158788\n",
      "-------------------------------------\n",
      "SOM training epoches 7\n",
      "neighborhood radius  1.6206565966927624\n",
      "-------------------------------------\n",
      "SOM training epoches 8\n",
      "neighborhood radius  1.3797296614612151\n",
      "-------------------------------------\n",
      "SOM training epoches 9\n",
      "neighborhood radius  1.1746189430880192\n",
      "-------------------------------------\n",
      "SOM training epoches 10\n",
      "neighborhood radius  1.0\n",
      "-------------------------------------\n"
     ]
    }
   ],
   "source": [
    "#https://machinelearningnepal.com/2018/01/22/apache-spark-implementation-of-som-batch-algorithm/\n",
    "# step 5 Som\n",
    "from som.batch_som import SOM\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib import patches as patches\n",
    "\n",
    "mapsize = calculate_map_size(input_data)\n",
    "print('Som: X = {} and Y = {}, {} neurones'.format(mapsize[0], mapsize[1],mapsize[0]*mapsize[1]))\n",
    "\n",
    "#x =  mapsize[0] \n",
    "#y = mapsize[1]\n",
    "som = SOM(10,5,21) # (xÃ—y) neurons, (70 features)\n",
    "som.train(scaledData, 10) # data , epochs  +10\n",
    "\n",
    "   # for show All codebook\n",
    "#som.net\n",
    "   # dimention of codebook (x,y,features)\n",
    "#print('dimention of codebook 3D:',som.net.shape)\n",
    "\n",
    "#som.net.reshpae(x*y,features)\n",
    "data = som.net\n",
    "data = data.transpose(2,1,0).reshape(-1,data.shape[2])\n",
    "#print('dimention of codebook 2D:',data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+----+---+---+---+----+-----+----+----+-----+-----+------+-----+-----+----+------+--------+---+--------+-----+--------------------+---------+--------------------+\n",
      "| AC|ALTV|ASTV| DL| DP| DS|  FM|   LB|MLTV|MSTV|  Max| Mean|Median|  Min| Mode|Nmax|Nzeros|Tendency| UC|Variance|Width|                 bmu|  bmu_idx|            features|\n",
      "+---+----+----+---+---+---+----+-----+----+----+-----+-----+------+-----+-----+----+------+--------+---+--------+-----+--------------------+---------+--------------------+\n",
      "|1.0| 6.0|54.0|0.0|0.0|0.0| 0.0|130.0| 8.7| 0.6|146.0|133.0| 134.0|116.0|133.0| 2.0|   0.0|     0.0|9.0|     1.0| 30.0|[0.83294447868712...|[2.0,0.0]|[0.8125,0.0384615...|\n",
      "|2.0|26.0|57.0|0.0|0.0|0.0|10.0|130.0|14.3| 0.6|156.0|131.0| 133.0| 65.0|129.0| 7.0|   0.0|     1.0|4.0|    10.0| 91.0|[0.83374089614232...|[8.0,4.0]|[0.8125,0.0769230...|\n",
      "+---+----+----+---+---+---+----+-----+----+----+-----+-----+------+-----+-----+----+------+--------+---+--------+-----+--------------------+---------+--------------------+\n",
      "only showing top 2 rows\n",
      "\n",
      "1688\n"
     ]
    }
   ],
   "source": [
    "# prediction som\n",
    "# pour avoir bmu, bmu, index \n",
    "pre = som.predict(scaledData)\n",
    "pre.show(2)\n",
    "print(pre.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---------+--------------------+\n",
      "|                 bmu|  bmu_idx|            features|\n",
      "+--------------------+---------+--------------------+\n",
      "|[0.83294447868712...|[2.0,0.0]|[0.8125,0.0384615...|\n",
      "|[0.83374089614232...|[8.0,4.0]|[0.8125,0.0769230...|\n",
      "|[0.78584788203453...|[0.0,2.0]|[0.6875,0.1153846...|\n",
      "|[0.78584788203453...|[0.0,2.0]|[0.6875,0.1923076...|\n",
      "|[0.88754497082603...|[9.0,1.0]|[0.85,0.115384615...|\n",
      "+--------------------+---------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# to kmenas, select, bmu \n",
    "bmux = pre.select('bmu','bmu_idx','features')\n",
    "print(bmux.show(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K-means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "from pyspark.ml.feature import VectorIndexer\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "# Automatically identify categorical features, and index them.\n",
    "# We specify maxCategories so features with > 4 distinct values are treated as continuous.\n",
    "\n",
    "featureIndexer = VectorIndexer(inputCol=\"bmu\", \\\n",
    "                               outputCol=\"indexedbmu\",\\\n",
    "                               ).fit(bmux)\n",
    "\n",
    "data = featureIndexer.transform(bmux)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---------+--------------------+--------------------+\n",
      "|                 bmu|  bmu_idx|            features|          indexedbmu|\n",
      "+--------------------+---------+--------------------+--------------------+\n",
      "|[0.83294447868712...|[2.0,0.0]|[0.8125,0.0384615...|[0.83294447868712...|\n",
      "|[0.83374089614232...|[8.0,4.0]|[0.8125,0.0769230...|[0.83374089614232...|\n",
      "|[0.78584788203453...|[0.0,2.0]|[0.6875,0.1153846...|[0.78584788203453...|\n",
      "|[0.78584788203453...|[0.0,2.0]|[0.6875,0.1923076...|[0.78584788203453...|\n",
      "|[0.88754497082603...|[9.0,1.0]|[0.85,0.115384615...|[0.88754497082603...|\n",
      "+--------------------+---------+--------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data.show(5,True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.clustering import KMeans, KMeansModel\n",
    "\n",
    "kmeans = KMeans() \\\n",
    "          .setK(2) \\\n",
    "          .setFeaturesCol(\"indexedbmu\")\\\n",
    "          .setPredictionCol(\"cluster\")\n",
    "\n",
    "# Chain indexer and tree in a Pipeline\n",
    "pipeline = Pipeline(stages=[featureIndexer, kmeans])\n",
    "\n",
    "model = pipeline.fit(bmux)\n",
    "\n",
    "cluster = model.transform(bmux)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster = model.transform(bmux)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---------+--------------------+--------------------+-------+\n",
      "|                 bmu|  bmu_idx|            features|          indexedbmu|cluster|\n",
      "+--------------------+---------+--------------------+--------------------+-------+\n",
      "|[0.83294447868712...|[2.0,0.0]|[0.8125,0.0384615...|[0.83294447868712...|      1|\n",
      "|[0.83374089614232...|[8.0,4.0]|[0.8125,0.0769230...|[0.83374089614232...|      0|\n",
      "|[0.78584788203453...|[0.0,2.0]|[0.6875,0.1153846...|[0.78584788203453...|      1|\n",
      "|[0.78584788203453...|[0.0,2.0]|[0.6875,0.1923076...|[0.78584788203453...|      1|\n",
      "|[0.88754497082603...|[9.0,1.0]|[0.85,0.115384615...|[0.88754497082603...|      0|\n",
      "+--------------------+---------+--------------------+--------------------+-------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cluster.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = cluster.select('cluster')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "y_pred = np.array(list(cluster.select('cluster').toPandas()['cluster']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "# load data\n",
    "\n",
    "df = pd.read_csv(\"Cardiotocography_02_v10.csv\")\n",
    "df['c90'] = df.c90.apply(lambda label: 1 if label == \"'yes'\" else 0)\n",
    "del df['id']\n",
    "del df['Unnamed: 0']\n",
    "# df to values\n",
    "df = df.values\n",
    "# \n",
    "y_test= df[:,21]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy of SOM K-means = 53.613744 %\n",
      "[[887 768]\n",
      " [ 15  18]]\n",
      "\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.98      0.54      0.69      1655\n",
      "         1.0       0.02      0.55      0.04        33\n",
      "\n",
      "   micro avg       0.54      0.54      0.54      1688\n",
      "   macro avg       0.50      0.54      0.37      1688\n",
      "weighted avg       0.96      0.54      0.68      1688\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# predict\n",
    "acc = accuracy_score(y_test, y_pred)\n",
    "print(\"Test Accuracy of SOM K-means = {:.6f} %\".format(acc * 100))\n",
    "# Matrix de confusion\n",
    "print(confusion_matrix(y_test,y_pred))\n",
    "print('\\n')\n",
    "print(classification_report(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
