{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://runawayhorse001.github.io/LearningApacheSpark/clustering.html\n",
    "from utilsom import * "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data = spark.read.csv(\"C:/Users/Principal/Desktop/htru.csv\", header=False, inferSchema=True, mode=\"DROPMALFORMED\", encoding='UTF-8')\n",
    "#df = df.drop(\"_c0\")\n",
    "#input_data.show(n=5,truncate=False)\n",
    "#http://archive.ics.uci.edu/ml/datasets/HEPMASS  \n",
    "#input_data = load_data(\"hdfs://localhost:9000/data/htru.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17898\n",
      "DataFrame[_c0: double, _c1: double, _c2: double, _c3: double, _c4: double, _c5: double, _c6: double, _c7: double, _c8: int]\n"
     ]
    }
   ],
   "source": [
    "print(input_data.count())\n",
    "print(input_data)\n",
    "\n",
    "# dellet label\n",
    "df_input_data = input_data.drop(\"_c8\")\n",
    "\n",
    "# staep 2\n",
    "df_input_data = shuffling(df_input_data, 10)\n",
    "\n",
    "# staep 3\n",
    "input_features = assembler(df_input_data,df_input_data.columns[0:])\n",
    "\n",
    "# staep 4\n",
    "scaledData = scaler(input_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SOM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SOM training epoches 1\n",
      "neighborhood radius  3.4822022531844965\n",
      "-------------------------------------\n",
      "SOM training epoches 2\n",
      "neighborhood radius  3.0314331330207964\n",
      "-------------------------------------\n",
      "SOM training epoches 3\n",
      "neighborhood radius  2.6390158215457884\n",
      "-------------------------------------\n",
      "SOM training epoches 4\n",
      "neighborhood radius  2.29739670999407\n",
      "-------------------------------------\n",
      "SOM training epoches 5\n",
      "neighborhood radius  2.0\n",
      "-------------------------------------\n",
      "SOM training epoches 6\n",
      "neighborhood radius  1.7411011265922482\n",
      "-------------------------------------\n",
      "SOM training epoches 7\n",
      "neighborhood radius  1.5157165665103982\n",
      "-------------------------------------\n",
      "SOM training epoches 8\n",
      "neighborhood radius  1.3195079107728944\n",
      "-------------------------------------\n",
      "SOM training epoches 9\n",
      "neighborhood radius  1.148698354997035\n",
      "-------------------------------------\n",
      "SOM training epoches 10\n",
      "neighborhood radius  1.0\n",
      "-------------------------------------\n",
      "dimention of codebook 3D: (6, 4, 8)\n",
      "dimention of codebook 2D: (24, 8)\n"
     ]
    }
   ],
   "source": [
    "#https://machinelearningnepal.com/2018/01/22/apache-spark-implementation-of-som-batch-algorithm/\n",
    "# step 5 Som\n",
    "from som.batch_som import SOM\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib import patches as patches\n",
    "\n",
    "#mapsize = calculate_map_size(input_data)\n",
    "#print('Som: X = {} and Y = {}, {} neurones'.format(mapsize[0], mapsize[1],mapsize[0]*mapsize[1]))\n",
    "\n",
    "#x =  mapsize[0] \n",
    "#y = mapsize[1]\n",
    "som = SOM(6,4,8) # (xÃ—y) neurons, (70 features)\n",
    "som.train(scaledData, 10) # data , epochs  +10\n",
    "\n",
    "   # for show All codebook\n",
    "#som.net\n",
    "   # dimention of codebook (x,y,features)\n",
    "print('dimention of codebook 3D:',som.net.shape)\n",
    "\n",
    "#som.net.reshpae(x*y,features)\n",
    "data = som.net\n",
    "data = data.transpose(2,1,0).reshape(-1,data.shape[2])\n",
    "print('dimention of codebook 2D:',data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----------+------------+------------+-----------+----------+-----------+-----------+--------------------+---------+--------------------+\n",
      "|        _c0|        _c1|         _c2|         _c3|        _c4|       _c5|        _c6|        _c7|                 bmu|  bmu_idx|            features|\n",
      "+-----------+-----------+------------+------------+-----------+----------+-----------+-----------+--------------------+---------+--------------------+\n",
      "|126.1640625|51.96750447| -0.18679188|-0.146062183|1.738294314|11.0873352|12.73873147|240.5595545|[0.66014276417499...|[4.0,3.0]|[0.65499898600689...|\n",
      "| 131.640625|49.76754313|-0.059808838|-0.490438395| 2.56270903|16.8883382|8.031904542|78.02820791|[0.64639010555872...|[3.0,2.0]|[0.68343135266680...|\n",
      "+-----------+-----------+------------+------------+-----------+----------+-----------+-----------+--------------------+---------+--------------------+\n",
      "only showing top 2 rows\n",
      "\n",
      "17898\n"
     ]
    }
   ],
   "source": [
    "# prediction som\n",
    "# pour avoir bmu, bmu, index \n",
    "pre = som.predict(scaledData)\n",
    "pre.show(2)\n",
    "print(pre.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---------+--------------------+\n",
      "|                 bmu|  bmu_idx|            features|\n",
      "+--------------------+---------+--------------------+\n",
      "|[0.66014276417499...|[4.0,3.0]|[0.65499898600689...|\n",
      "|[0.64639010555872...|[3.0,2.0]|[0.68343135266680...|\n",
      "|[0.51171312812475...|[4.0,0.0]|[0.53920097343338...|\n",
      "|[0.67356597521172...|[2.0,3.0]|[0.64173595619549...|\n",
      "|[0.59848563614992...|[3.0,1.0]|[0.60373149462583...|\n",
      "+--------------------+---------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# to kmenas, select, bmu \n",
    "bmux = pre.select('bmu','bmu_idx','features')\n",
    "print(bmux.show(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K-means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "from pyspark.ml.feature import VectorIndexer\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "# Automatically identify categorical features, and index them.\n",
    "# We specify maxCategories so features with > 4 distinct values are treated as continuous.\n",
    "\n",
    "featureIndexer = VectorIndexer(inputCol=\"bmu\", \\\n",
    "                               outputCol=\"indexedbmu\",\\\n",
    "                               ).fit(bmux)\n",
    "\n",
    "data = featureIndexer.transform(bmux)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---------+--------------------+--------------------+\n",
      "|                 bmu|  bmu_idx|            features|          indexedbmu|\n",
      "+--------------------+---------+--------------------+--------------------+\n",
      "|[0.66014276417499...|[4.0,3.0]|[0.65499898600689...|[0.66014276417499...|\n",
      "|[0.64639010555872...|[3.0,2.0]|[0.68343135266680...|[0.64639010555872...|\n",
      "|[0.51171312812475...|[4.0,0.0]|[0.53920097343338...|[0.51171312812475...|\n",
      "|[0.67356597521172...|[2.0,3.0]|[0.64173595619549...|[0.67356597521172...|\n",
      "|[0.59848563614992...|[3.0,1.0]|[0.60373149462583...|[0.59848563614992...|\n",
      "+--------------------+---------+--------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data.show(5,True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.clustering import KMeans, KMeansModel\n",
    "\n",
    "kmeans = KMeans() \\\n",
    "          .setK(2) \\\n",
    "          .setFeaturesCol(\"indexedbmu\")\\\n",
    "          .setPredictionCol(\"cluster\")\n",
    "\n",
    "# Chain indexer and tree in a Pipeline\n",
    "pipeline = Pipeline(stages=[featureIndexer, kmeans])\n",
    "\n",
    "model = pipeline.fit(bmux)\n",
    "\n",
    "cluster = model.transform(bmux)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster = model.transform(bmux)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---------+--------------------+--------------------+-------+\n",
      "|                 bmu|  bmu_idx|            features|          indexedbmu|cluster|\n",
      "+--------------------+---------+--------------------+--------------------+-------+\n",
      "|[0.66014276417499...|[4.0,3.0]|[0.65499898600689...|[0.66014276417499...|      0|\n",
      "|[0.64639010555872...|[3.0,2.0]|[0.68343135266680...|[0.64639010555872...|      0|\n",
      "|[0.51171312812475...|[4.0,0.0]|[0.53920097343338...|[0.51171312812475...|      0|\n",
      "|[0.67356597521172...|[2.0,3.0]|[0.64173595619549...|[0.67356597521172...|      0|\n",
      "|[0.59848563614992...|[3.0,1.0]|[0.60373149462583...|[0.59848563614992...|      0|\n",
      "+--------------------+---------+--------------------+--------------------+-------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cluster.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = cluster.select('cluster')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "y_pred = np.array(list(cluster.select('cluster').toPandas()['cluster']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "# load data\n",
    "import names\n",
    "df = pd.read_csv(\"htru.csv\", names = names.names)\n",
    "# df to values\n",
    "df = df.values\n",
    "# \n",
    "y_test = df[:,8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy of SOM K-means = 86.657727 %\n",
      "[[15421   838]\n",
      " [ 1550    89]]\n",
      "\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.91      0.95      0.93     16259\n",
      "         1.0       0.10      0.05      0.07      1639\n",
      "\n",
      "   micro avg       0.87      0.87      0.87     17898\n",
      "   macro avg       0.50      0.50      0.50     17898\n",
      "weighted avg       0.83      0.87      0.85     17898\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# predict\n",
    "acc = accuracy_score(y_test, y_pred)\n",
    "print(\"Test Accuracy of SOM K-means = {:.6f} %\".format(acc * 100))\n",
    "# Matrix de confusion\n",
    "print(confusion_matrix(y_test,y_pred))\n",
    "print('\\n')\n",
    "print(classification_report(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
