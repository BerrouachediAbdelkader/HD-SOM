{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://runawayhorse001.github.io/LearningApacheSpark/clustering.html\n",
    "from utilsom import * "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1013\n",
      "DataFrame[_c0: int, att1: double, att2: double, att3: double, att4: double, att5: double, att6: double, att7: double, att8: double, att9: double, id: double, outlier: string]\n"
     ]
    }
   ],
   "source": [
    "input_data = spark.read.csv(\"Shuttle_withoutdupl_norm_v06.csv\", header=True, inferSchema=True, mode=\"DROPMALFORMED\", encoding='UTF-8')\n",
    "#df = df.drop(\"_c0\")\n",
    "#input_data.show(n=5,truncate=False)\n",
    "#http://archive.ics.uci.edu/ml/datasets/HEPMASS  \n",
    "#input_data = load_data(\"hdfs://localhost:9000/data/htru.csv\")\n",
    "\n",
    "print(input_data.count())\n",
    "print(input_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# dellet label\n",
    "df_input_data = input_data.drop(\"_c0\",\"outlier\",\"id\")\n",
    "\n",
    "# staep 2\n",
    "df_input_data = shuffling(df_input_data, 10)\n",
    "\n",
    "# staep 3\n",
    "input_features = assembler(df_input_data,df_input_data.columns[0:])\n",
    "\n",
    "# staep 4\n",
    "scaledData = scaler(input_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SOM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Principal\\Desktop\\these\\SOM\\SomSpark\\som chine\\Shuttle\\utilsom.py:81: RuntimeWarning: invalid value encountered in add\n",
      "  A = np.ndarray(shape=[dim, dim]) + np.Inf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Som: X = 14 and Y = 11, 154 neurones\n",
      "SOM training epoches 1\n",
      "neighborhood radius  13.481718494401385\n",
      "-------------------------------------\n",
      "SOM training epoches 2\n",
      "neighborhood radius  10.097596309015795\n",
      "-------------------------------------\n",
      "SOM training epoches 3\n",
      "neighborhood radius  7.562941717125411\n",
      "-------------------------------------\n",
      "SOM training epoches 4\n",
      "neighborhood radius  5.664525067769413\n",
      "-------------------------------------\n",
      "SOM training epoches 5\n",
      "neighborhood radius  4.242640687119286\n",
      "-------------------------------------\n",
      "SOM training epoches 6\n",
      "neighborhood radius  3.1776715231464365\n",
      "-------------------------------------\n",
      "SOM training epoches 7\n",
      "neighborhood radius  2.3800262745964416\n",
      "-------------------------------------\n",
      "SOM training epoches 8\n",
      "neighborhood radius  1.7826024579660036\n",
      "-------------------------------------\n",
      "SOM training epoches 9\n",
      "neighborhood radius  1.3351413625403128\n",
      "-------------------------------------\n",
      "SOM training epoches 10\n",
      "neighborhood radius  1.0000000000000002\n",
      "-------------------------------------\n",
      "dimention of codebook 3D: (18, 20, 9)\n",
      "dimention of codebook 2D: (360, 9)\n"
     ]
    }
   ],
   "source": [
    "#https://machinelearningnepal.com/2018/01/22/apache-spark-implementation-of-som-batch-algorithm/\n",
    "# step 5 Som\n",
    "from som.batch_som import SOM\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib import patches as patches\n",
    "\n",
    "mapsize = calculate_map_size(input_data)\n",
    "print('Som: X = {} and Y = {}, {} neurones'.format(mapsize[0], mapsize[1],mapsize[0]*mapsize[1]))\n",
    "\n",
    "#x =  mapsize[0] \n",
    "#y = mapsize[1]\n",
    "som = SOM(18,20,9) # (xÃ—y) neurons, (70 features)\n",
    "som.train(scaledData, 10) # data , epochs  +10\n",
    "\n",
    "   # for show All codebook\n",
    "#som.net\n",
    "   # dimention of codebook (x,y,features)\n",
    "print('dimention of codebook 3D:',som.net.shape)\n",
    "\n",
    "#som.net.reshpae(x*y,features)\n",
    "data = som.net\n",
    "data = data.transpose(2,1,0).reshape(-1,data.shape[2])\n",
    "print('dimention of codebook 2D:',data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+--------------+--------------+---------------+--------------+---------------+--------------+--------------+--------------+--------------------+---------+--------------------+\n",
      "|           att1|          att2|          att3|           att4|          att5|           att6|          att7|          att8|          att9|                 bmu|  bmu_idx|            features|\n",
      "+---------------+--------------+--------------+---------------+--------------+---------------+--------------+--------------+--------------+--------------------+---------+--------------------+\n",
      "|0.0114942528736|0.272727272727|0.702127659574|0.0519761775853|0.193181818182|0.0121744791667|0.923076923077|0.843260188088|0.711340206186|[0.01267699824735...|[1.0,5.0]|[0.0114942528736,...|\n",
      "|0.0114942528736|0.251336898396|0.404255319149|0.0508933405522|0.136363636364|0.0127604166667| 0.78021978022|0.865203761755|0.762886597938|[0.01663534969058...|[4.0,0.0]|[0.0114942528736,...|\n",
      "+---------------+--------------+--------------+---------------+--------------+---------------+--------------+--------------+--------------+--------------------+---------+--------------------+\n",
      "only showing top 2 rows\n",
      "\n",
      "1013\n"
     ]
    }
   ],
   "source": [
    "# prediction som\n",
    "# pour avoir bmu, bmu, index \n",
    "pre = som.predict(scaledData)\n",
    "pre.show(2)\n",
    "print(pre.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----------+--------------------+\n",
      "|                 bmu|    bmu_idx|            features|\n",
      "+--------------------+-----------+--------------------+\n",
      "|[0.01267699824735...|  [1.0,5.0]|[0.0114942528736,...|\n",
      "|[0.01663534969058...|  [4.0,0.0]|[0.0114942528736,...|\n",
      "|[0.21484256847656...| [8.0,17.0]|[0.218390804598,0...|\n",
      "|[0.18673934652225...|[10.0,14.0]|[0.183908045977,0...|\n",
      "|[0.01276592229651...| [16.0,4.0]|[0.0114942528736,...|\n",
      "+--------------------+-----------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# to kmenas, select, bmu \n",
    "bmux = pre.select('bmu','bmu_idx','features')\n",
    "print(bmux.show(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K-means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "from pyspark.ml.feature import VectorIndexer\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "# Automatically identify categorical features, and index them.\n",
    "# We specify maxCategories so features with > 4 distinct values are treated as continuous.\n",
    "\n",
    "featureIndexer = VectorIndexer(inputCol=\"bmu\", \\\n",
    "                               outputCol=\"indexedbmu\",\\\n",
    "                               ).fit(bmux)\n",
    "\n",
    "data = featureIndexer.transform(bmux)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----------+--------------------+--------------------+\n",
      "|                 bmu|    bmu_idx|            features|          indexedbmu|\n",
      "+--------------------+-----------+--------------------+--------------------+\n",
      "|[0.01267699824735...|  [1.0,5.0]|[0.0114942528736,...|[0.01267699824735...|\n",
      "|[0.01663534969058...|  [4.0,0.0]|[0.0114942528736,...|[0.01663534969058...|\n",
      "|[0.21484256847656...| [8.0,17.0]|[0.218390804598,0...|[0.21484256847656...|\n",
      "|[0.18673934652225...|[10.0,14.0]|[0.183908045977,0...|[0.18673934652225...|\n",
      "|[0.01276592229651...| [16.0,4.0]|[0.0114942528736,...|[0.01276592229651...|\n",
      "+--------------------+-----------+--------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data.show(5,True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.clustering import KMeans, KMeansModel\n",
    "\n",
    "kmeans = KMeans() \\\n",
    "          .setK(2) \\\n",
    "          .setFeaturesCol(\"indexedbmu\")\\\n",
    "          .setPredictionCol(\"cluster\")\n",
    "\n",
    "# Chain indexer and tree in a Pipeline\n",
    "pipeline = Pipeline(stages=[featureIndexer, kmeans])\n",
    "\n",
    "model = pipeline.fit(bmux)\n",
    "\n",
    "cluster = model.transform(bmux)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster = model.transform(bmux)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----------+--------------------+--------------------+-------+\n",
      "|                 bmu|    bmu_idx|            features|          indexedbmu|cluster|\n",
      "+--------------------+-----------+--------------------+--------------------+-------+\n",
      "|[0.01267699824735...|  [1.0,5.0]|[0.0114942528736,...|[0.01267699824735...|      1|\n",
      "|[0.01663534969058...|  [4.0,0.0]|[0.0114942528736,...|[0.01663534969058...|      1|\n",
      "|[0.21484256847656...| [8.0,17.0]|[0.218390804598,0...|[0.21484256847656...|      1|\n",
      "|[0.18673934652225...|[10.0,14.0]|[0.183908045977,0...|[0.18673934652225...|      1|\n",
      "|[0.01276592229651...| [16.0,4.0]|[0.0114942528736,...|[0.01276592229651...|      1|\n",
      "+--------------------+-----------+--------------------+--------------------+-------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cluster.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = cluster.select('cluster')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "y_pred = np.array(list(cluster.select('cluster').toPandas()['cluster']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "# load data\n",
    "\n",
    "df = pd.read_csv(\"Shuttle_withoutdupl_norm_v06.csv\")\n",
    "del df['id']\n",
    "del df['Unnamed: 0']\n",
    "df['outlier'] = df.outlier.apply(lambda label: 1 if label == \"'yes'\" else 0)\n",
    "\n",
    "# df to values\n",
    "df = df.values\n",
    "# \n",
    "y_test = df[:,9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy of SOM K-means = 15.301086 %\n",
      "[[144 856]\n",
      " [  2  11]]\n",
      "\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.99      0.14      0.25      1000\n",
      "         1.0       0.01      0.85      0.02        13\n",
      "\n",
      "   micro avg       0.15      0.15      0.15      1013\n",
      "   macro avg       0.50      0.50      0.14      1013\n",
      "weighted avg       0.97      0.15      0.25      1013\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# predict\n",
    "acc = accuracy_score(y_test, y_pred)\n",
    "print(\"Test Accuracy of SOM K-means = {:.6f} %\".format(acc * 100))\n",
    "# Matrix de confusion\n",
    "print(confusion_matrix(y_test,y_pred))\n",
    "print('\\n')\n",
    "print(classification_report(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
